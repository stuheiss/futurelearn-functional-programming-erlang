0:03Skip to 0 minutes and 3 secondsJOE ARMSTRONG: In the last lecture, I showed you the basic primitives that we can use to build concurrent programs. We have things like spawn, and send, receive and so on. But I didn't show you how they could be used. So what I'm going to do in this lecture is build some abstractions that turn out to be useful for building concurrent programs with. Before I do that, I'm going to return to sequential programming and remind you of some simple higher order functions, or rather a simple higher order function. Erlang does not have a for loop.

0:35Skip to 0 minutes and 35 secondsBut we can easily make one. Making one is a higher order function. Higher order functions are functions that either return functions or they have functions as their arguments. This is a higher order function that becomes a for loop. And it consists of two clauses, for(Max,Max,F) returns a list [F(Max)]; for(I,Max,F) returns the list... F(I), that's the head of the list, and the tail of the list is for(I+1,Max,F). It's a very simple function. And it's called as in the example on the slide. If we say for of 1, 5, fun(I) is I times I. And it returns the list [1,4,9,16,25]. They're pretty much like a for loop that you would find in any conventional programming language. But that's a sequential abstraction.

1:18Skip to 1 minute and 18 secondsWhat we can do is we can build concurrent abstractions. So the first one I'm going to do is build a remote procedure call. And this is one way that we can build a remote procedure call. There are actually several ways we can do that. And they take care of different error cases. So what is a remote procedure call? A remote procedure call is something you can use on a local computer. It will make a local function call look like a remote function call. What does it consists of? It consists of sending a message to a remote computer, and then waiting for a reply to come back from that computer. So constructing a remote procedure call is pretty easy.

1:55Skip to 1 minute and 55 secondsIt takes two arguments, a Pid and the request. A Pid conventionally stands for process identifier. It's the name of the process that's going to handle the request. We send the message to that. And that message has to contain our own identity. If it doesn't contain our own identity, the remote computer won't know who to send the message back to. And that's supplied by this parameter called self. And then we generate a unique tag.

2:21Skip to 2 minutes and 21 secondsUnique tags done with this line of code erlang:make_ref() And the purpose of that is to pattern match on it when the message comes back. We don't want to send a request to a computer and then get some other message and to not associate the correct response with the correct message. And then we wait for a message here that has Tag as its first argument. And then we return Response. That's pretty simple. What could go wrong? Well, we don't want to wait forever. So the next thing we could do is we could add a timeout to that. So we could say same code as before, and we've just added after time.

3:00Skip to 3 minutes and 0 seconds And if the time that's shown in the slide has elapsed, then we will execute some different code. We can play tricks with this remote procedure code. One thing we can do is we can split it into two. The remote procedure call actually consists of two parts. It consists of a part which sends a message to a remote process. And it consists of a part which receives a message. So this code is split into two. Instead of writing out rpc as one function, we've written it as two functions.

3:37Skip to 3 minutes and 37 secondsWe said rpc(Pid, Request) is, Tag is erlang:make_ref(), we send the message and we return Tag. And in the second clause we say wait_response argument Tag. And that waits for a message whose value is a tuple with two arguments. The first argument being Tag. OK, that might look unfamiliar to you but if we rename it like that, we can rename the first part as promise, and the second part as yield. So we've invented a construction which we call futures. Futures are things which are hard wired into several programming languages. They consist of a promise and yield step, and you can use them to write concurrent programs. In Erlang, they're not built into the language.

4:22Skip to 4 minutes and 22 seconds They can just be easily thrown together in two to three lines of code. So this adheres to our general principles. We do not provide built in mechanisms, we provide primitives with which mechanisms can be built. And so in this example, we have constructed futures from the primitives. The primitives involved here make_ref, sending a message and receiving a message. So how would you use that? You'd say Tag is a promise. You give it to some function it's going to compute. You do some computation. When that computation is over, you can say value is yield(Tag). But are promises and futures, are they nice abstractions to program with? Well, some people think they are. Others don't.

5:09Skip to 5 minutes and 9 secondsSome other languages have a par do, par end construction. Parallel do. So in languages like Occam, you'll find a par begin, par end, and a sequence of statements. And this construct in the language means do these statements F1, F2, and F3 in parallel. Can we make such an abstraction in Erlang? Well yes, it's rather easy. Let's do it like this. We can define this as pmap. It's actually a parallel version of a mapping function and it consists of three lines of code. It dispatches the parallel computation to three lines of code-- actually two lines. S is self. This is the identity of the process that's going to receive the answers.

5:56Skip to 5 minutes and 56 seconds Pids is this function do(S, F), well, that's mapped over the arguments of a list. And then we go in to receive statement with the arguments that come from the list of Pids. This involves several things. It involves a couple of list comprehensions. It involves a spawn and send and receive. But having done that, we've made an abstraction, which allows us to easily describe parallel computations.

